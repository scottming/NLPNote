{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/dev/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哪怕是经验非常丰富的深度学习专家也不可能一开始就预设出最匹配的超级参数，所以说，应用深度学习是一个典型的迭代过程。\n",
    "\n",
    "通常来说，会把数据集划分为三个部分：\n",
    "\n",
    "* 训练集(training set)\n",
    "* 验证集(hold-out/development set/dev)\n",
    "* 测试集(test set)\n",
    "\n",
    "用训练集做模型训练，通过验证集或简单交叉验证集选择最好的模型\n",
    "\n",
    "机器学习的小数据量时代，常见做法是将所有数据三七分，也就是 70/30 train test splits，如果没有明确的验证集，也可以按照 60/20/20(train/dev/tst) 的分法。在万级别以下，用上面那个分法都是可以的，但是大数据时代，如果是百万级的数据，那么验证集合测试集占数据总量的比例会趋向于变得更小，因为验证集和测试集都是为了检验哪种算法更有效果。\n",
    "\n",
    "深度学习的另一个趋势是越来越在训练集和测试集的数据不匹配的情况了，针对这种情况，要确保验证集和测试集的数据来自同一分布。\n",
    "\n",
    "另外，哪怕是没有测试集也是可以的，测试集的目的是为了确保最终选定的神经网络系统作出无偏估计，不需要无偏估计的时候就可以不需要测试集，这个时候，就会把训练集叫 training set，验证集叫 test set，而实际上叫 train/dev set 更为专业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance(偏差、方差)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个概念易学难精，还有个概念是 bias-variance trade-off，我们总是分别讨论偏差或方差，但却很少谈及偏差和方差的权衡问题。\n",
    "\n",
    "理解偏差和方差的两个关键数据是训练集误差和验证集误差。\n",
    "\n",
    "比如，当训练误差很小(1%)，但验证误差(11%)却很高的时候，我们便清晰的知道模型过拟合了，我们便说这是高方差(high variances)；当训练集(15%)和验证集(16%)误差都比较高的时候，这个时候模型没有得到充分的训练，我们说这个情况是欠拟合，也是高偏差(high bias);当训练集(15%)的误差很高，便已经是偏差高了，同时验证集的误差(30%)更高时，这就可以说是高偏差+高方差。\n",
    "\n",
    "特性|high variances|high bias|high bias & high variances|low bias & low variances\n",
    "---|---|---|---|---\n",
    "Train set error|1% |15%|15%|0.5%\n",
    "Dev set error  |11%|16%|30%|1%\n",
    "\n",
    "当然以上分析是建立在最优误差很小，以及训练集和验证集是来自统一分布的情况下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic recipe for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成模型后，我们首先要知道的是算法的偏差高不高？因为如果偏差很高，那我们可以需要换新的网络，或者增加隐层，增长训练时间等等。反复尝试，知道可以拟合数据为止。一旦 bias 到了一个可以接受的程度，那么就可以开始评估方差了，为了评估方差，我们要查看验证集性能，如果有很高的方差，那么最好的解决办法就是采用更多数据，但是有的时候我们无法获得更多数据，我们可以通过正则化来减少过拟合，有的时候得反复尝试，而有的时候能找到合适的神经网络，则能一箭双雕。只能不断尝试，知道找到一个低偏差、低方差的框架。\n",
    "\n",
    "有两点必须注意：\n",
    "\n",
    "1. 高偏差和高方差是两种不同的情况。我们后续要尝试的方法也可能不同。所以大家要很清楚存在问题是偏差还是方差，还是两者都有问题，比如说偏差很高，那么通常准备更多数据也无用。\n",
    "2. 在机器学习的初期阶段，关于所谓的偏差方差的权衡探讨屡见不鲜，原因是我们能尝试的方案有很多，可以增加偏差，减少方差，也可以减少偏差，减少方差，但是在深度学习的早起阶段，我们没有太多工具可以做到，值减少偏差或方差，却不影响到另外一方。但是在当前的大数据时代，只要持续训练一个更大的网络，准备更多的数据，那么也并发只有这两种情况。我们假定这样的情况，只要正则适度，通常构建一个更大的网络便可以在不影响方差的同时，减少偏差，而采用更大数据则通常可以在不影响偏差的同时，减少方差。这两步实际上做的是，训练网络、选择网络或者准备更多数据。\n",
    "\n",
    "吴恩达觉得，现在我们有工具可以做到在减少一方的同时，而不影响另外一方，这应该就是深度学习对监督式学习大有裨益的一个重要原因，也是我们不用太过关注如何平衡偏差和方差的一个重要原因。\n",
    "\n",
    "训练一个更大的神经网络几乎没有任何负面影响，而训练一个大型神经网络的主要代价也只是计算时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization(正则化)："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它是一种非常实用的减少方差的方法。正则化会出现偏差增大的权衡问题，但如果网络足够大，增幅通常不会太高。\n",
    "\n",
    "如果怀疑过拟合，那么第一个想到的办法就是正则化，另一个解决办法就是准备更多数据，这也是非常可靠的办法。\n",
    "\n",
    "针对 logistic 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\|w\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个也叫 $L_2$ 正则化\n",
    "\n",
    "为啥只写 $w$，不带 $b$ 呢？因为 $w$ 是个高维参数，几乎涵盖了所有参数，而 $b$ 只是个常数\n",
    "\n",
    "如果用的是 $L_1$ 正则，$w$ 最终会是稀疏的，也就是说 $w$ 向量中有很多 0，有人说这样可以压缩模型，实际上，虽然 $L_1$ 正则化使模型变得稀疏，但并不会降低多少内存，目前是 $L_2$ 用的越来越频繁了。\n",
    "\n",
    "$\\lambda$ 是正则化的参数，通常用验证集或者交叉验证集来配置这个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的正则化叫 Frobenius norm，可以参考 Tensorflow p87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(w^{[1]}, b^{[1]}, ..., w^{[l]},b^{[l]}) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{l=1}^{l}\\|w\\|_F^2$$\n",
    "\n",
    "$$ \\|w^{[l]}\\|_F^2 = \\sum_{i=1}^{n^{l-1}}\\sum_{j=1}^{n^{l}}(w_{ij}^{[l]})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why regularization reduces overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么正则化，可以减少过拟合呢？直观上理解就是如果正则化 $\\lambda$ 设置的足够大，那么权重矩阵 w 会被设置为接近 0 的值，也就是把多隐藏单元的权重设为了 0，于是产本消除了这些隐藏单元的许多影响，而这个被大大简化了的神经网络会变成一个很小的网络，所以这是一个从右图往左图的一个过程，让网络变得更加简单了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直观上理解，很多隐藏单元被消除了，实际上是该神经网络的所有隐藏单元依然存在，但是他们的影响变得更小了，神经网络变得更简单了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 tanh 函数来理解，则是，当 $\\lambda$ 增大的话，激活函数的参数会相对小，因为代价函数的参数变大了。如果 w 变小了，那么 z 也会很小，如果 z 比较小，那么就会在比较陡的那个范围，g(z) 则大致呈线性了。这样的话，每层就呈线性了，这样即使是非常深的神经网络，因为具有线性激活函数的特征，最终我们只能计算线性函数，因此它不适用于非常复杂的决策，以及过度拟合数据集的非显卡决策边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在增加正则化项时，应用之前定义的代价函数 J，当使用正则化函数时，J 已经有了一个全新的定义。如果只是用的原先的代价函数 J，那么将看不到单调递减现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout 会遍历神经网络的每一层，并设置消除神经网络中节点的概率，最后会得到一个规模更小的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing dropout 的几种方法：\n",
    "\n",
    "* Inverted dropout (目前最常用的 dropout 方法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multply(a3, d3)  # a3 *= d3  # 这里可以把为 0 的元素消除\n",
    "a3 /= keep_prob  # 这里没太看懂\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试阶段不使用 dropout，因为在测试阶段，我们不期望输出结果是随机的。(这点也没太看懂)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解 dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对 dropout 直观的理解是：好像每次迭代之后，神经网络都会变得比与以前更小，因此采用一个较小神经网络好像和使用正则化的效果是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下：\n",
    "\n",
    "如果你担心某些层比其他层更容易发生过拟合，可以把某些层的 keep-prob 值设置的比其他层更低，缺点是参数过大，设置变得更复杂。还有一种方法是，有些层使用 dropout，有些层不使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout 在视觉领域用的非常频繁。当然了，除非出现过拟合，否则不要使用 dropout。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout 的一大缺点就是代价函数 J 不再被明确定义，所以通常都是关闭 dropout 后，看看梯度有没下降，然后再打开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 其他正则方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据集不足的时候，可以考虑在元佑的数据上做扩充，也可以提早结束训练，但有个缺点是，这是用一种方法同时解决两个问题( J 下降和没有过拟合)，很有可能都解决不好。而用 $L_2$ 正则化则要运行很多次，去确认合适的 $\\lambda$(吴恩达比较倾向于这个)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 正则化输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化输入有两个步骤：\n",
    "\n",
    "1. 零均值化 $\\mu = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}$\n",
    "2. 归一化方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示一下：如果用它来调整训练数据，那么用相同的 $\\mu$ 和 $\\sigma^2$ 来归一化测试集，而不是在训练集和测试集分别估计这两个值。\n",
    "\n",
    "之所以归一化输入，是因为如果不归一化，那么代价函数会变得非常狭长。而归一化后，会变得更加容易优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 梯度消失和梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，激活函数以指数级递减，直观的理解是：权重值比 1 略大一点，或者说只比单位矩阵稍微大一些，深度神经网络的激活函数将爆炸式增长，如果 w 比 1 略小一点，在深度学习网络中，激活函数将以指数级递减。\n",
    "\n",
    "这样会导致训练难度加大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 权重的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度消失和梯度爆炸让我们知道，初始化权重是很重要的。\n",
    "\n",
    "\n",
    "这里讲的一些点比较重要，但没太看懂，等看过书和资料后，再来补下笔记。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 梯度的数值逼近"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有一个测试叫做梯度检验，他确保 backprob 在正确实施.\n",
    "\n",
    "这一章也美太看懂，需要推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 梯度检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad check 的主要是为了确认求导是否正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14 关于梯度检验的实施"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何在神经网络实施梯度检验的使用技巧和注意事项：\n",
    "\n",
    "1. 不要在训练中使用梯度检验，他只用于调试，计算所有 $i$ 值的 $d\\theta approx[i]$ 是非常漫长的，所以，为了实施梯度下降，你必须使用 backprop 来计算 $d\\theta$，并使用 backprop 来计算导数，而只有在 debug 的时候，采取计算那个，来确认数值是否接近 $d\\theta$，完成后，需要关闭它。\n",
    "2. 如果算法的梯度检验失败，要检查所有项，检查每一项，试着找出 bug，也就是说，如果 $d\\theta approx[i]$ 与 $d\\theta$ 的值相差很大，检查不同的 $i$ 值，看看哪个导数会导致他们相差那么大。这可以让我们知道在哪个地方 debug。\n",
    "3. 在实施梯度检验时，如果使用正则化。\n",
    "4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中 dropout 会随机消除隐层单元的 \n",
    "不同子集，难以计算代价函数 J，你只是对成本函数做抽样。所以很难用梯度检验来双重检验 dropout 的计算。不要同时使用这两者。   \n",
    "5. 「你需要做一件事，我不经常这么做，就是在随机处处长话过程中，运行梯度检验，然后在训练神经网络」这点没太看懂。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
