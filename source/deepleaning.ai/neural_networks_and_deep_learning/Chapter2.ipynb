{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 二分分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的惯例符号能够将不同训练样本的数据联系起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic Regression 非常适合解决二元分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "符号约定:\n",
    "    \n",
    "我们通常会把 w 和 参数 b 分开,b 对应一个拦截器，有一些课程是把 w 和 b 放一起了编程 $\\theta$，但在神经网络中，还是分开好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 logistic Regression cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了一个损失函数 L，来衡量你的预测值 y-hat 和 y 的实际值有多接近，但在 logistic Regression 中，如果用误差平方和，会是一个非凸函数，导致局部最优，这样梯度下降法就会很不好用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\hat{y}) = - (y log \\hat{y} + (1-y)log(1-\\hat{y}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么这个 loss 函数有用呢？待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，Loss Function 是在单个样本中定义的，他衡量了在单个训练样本上的表现。下面定义个一个成本函数(Cost Function)，他衡量的是在全体训练样本上的表现，他跟 Loss Function 是相关联的。公式如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(w, b) \n",
    "= \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)}) \n",
    "= - \\frac{1}{m}\\sum_{i=1}^{m}[(y log \\hat{y} + (1-y)log(1-\\hat{y}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 $w, b$ 都是一个实数的时候，$J(w, b)$ 就是一个曲面了\n",
    "\n",
    "待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为函数是凸函数，无论在那里初始化，都应该达到同一点，或大致相同的点。\n",
    "\n",
    "梯度下降法所做的就是，从初始点开始，朝最抖的地方往下坡方向走一步，也就是下降最快的方向走，直至最低点。\n",
    "\n",
    "导数是函数在一个点的斜率，所以他是变化的，在图的右边是正的，而在图的左边是负的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于符号：\n",
    "\n",
    "花俏的导数符号，是有两个变量以上的时候才用，这个叫偏导数符号 $\\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 导数简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导数的定义是你右移一个不可度量的值， 一个无限小的的值，$f(a)$ 就会增加，增加了一个非常非常小的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 更多导数的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(a) = a^2$ 的导数是变化的，其实在一个曲线上，画上三角形，高和宽的比值是不一样的(高是 Y 变化的量，宽则是 x 变化的量)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 $f(a) = log_e (a)$，$\\frac{d}{da} f(a) = \\frac{1}{a}$，因为 log 的时候，哪怕 $x$ 的变化非常大，$y$ 的变化却是越来越小，所以我想，这才是 $log$ 用的如此频繁的原因吧。\n",
    "\n",
    "小结：导数即斜率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个神经网络的计算都是按照前向或反向传播过程来实现的，首先计算出神经网络的输出，紧接着进行一个反向传输操作，后者我们用来计算出对应的梯度或者导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(a, b, c) = 3(a + b*c)$\n",
    "\n",
    "计算这个函数其实有三个不同的步骤：\n",
    "\n",
    "1. $u = b*c$\n",
    "2. $v = a+u$\n",
    "3. $J = 3*v$\n",
    "\n",
    "计算图待补<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从左到右可以计算一个值，而从右到左又是计算导数的最自然的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 计算图的导数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xjuve.com1.z0.glb.clouddn.com/170901-ComputingDerivatives.png?imageView2/2/w/800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = 3v$, $J$ 对 $v$ 的导数是 $\\frac{dJ}{dv} = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播的属于中，如果你想计算最后输出变量的导数，使用你最关心的变量对 v 的导数，那么我们就昨晚了一步反向传播。那么 $\\frac{dJ}{da}$ 是多少呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在微积分中，这叫链式法则，J 受 v 的影响，而 v 受 a 的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a = 5$ -> 5.001    \n",
    "$v = 11$ -> 11.001    \n",
    "$J = 33$ -> 33.003    \n",
    "\n",
    "$\\frac{dJ}{da} = 3 = \\frac{dJ}{dv}\\frac{dv}{da} = 3 * 1$\n",
    "\n",
    "$\\frac{dJ}{du} = 3 = \\frac{dJ}{dv}\\frac{dv}{du}$\n",
    "\n",
    "$\\frac{dJ}{db} = \\frac{dJ}{du}\\frac{du}{db} = 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算这些导数最有效率的办法是：\n",
    "\n",
    "从右往左计算，特别是，当我们第一次计算对 V 的导数时，之后再计算对 a 导数就可以用到，对 u 的导数可以帮助计算对 b 的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "符号约定：\n",
    "\n",
    "当你编程实现反向传播时，通常会有一个最终输出值是你要关心的.\n",
    "\n",
    "FinalOutput 一般是 J，计算针对某一个变量时，符号一般是 dJdvar 缩减至 -> dvar，如 da dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Logistic 回归中的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "通过 $da$ -> $dz$ -> $dw_1$、$dw_2$、$db$，算出了这些，便可以做梯度更新：\n",
    "\n",
    "$w_1 := w_1 - \\alpha dw_1$    \n",
    "$w_2 := w_2 - \\alpha dw_2$    \n",
    "$b := b - \\alpha b$\n",
    "\n",
    "上面是单个样本的梯度更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.10 M 个样本的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "首先要记得 J 的定义：\n",
    "\n",
    "$J(w, b) = \\frac{1}{m} L(a^{(i)}, y)$\n",
    "\n",
    "他就是关于一个损失的平均，同样的那么导数也是导数的平均：\n",
    "\n",
    "$\\frac{\\delta}{\\delta w_i}J(w, b) = \\frac{1}{m}\\sum_{i=1}^m \\frac{\\delta}{\\delta w_i} L(a^{(i)}, y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化，是消除你的代码中显式 for 循环语句的艺术，在深度学习领域，在训练大数据集时，深度学习算法表现才更加优越。所以代码运行的非常快非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非向量化对比向量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249955.246587\n",
      "Vectorized version: 1.2259483337402344 ms\n",
      "249955.246587\n",
      "For loop: 510.6618404388428 ms\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(f\"Vectorized version: {1000 * (toc - tic)} ms\")\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(f\"For loop: {1000 * (toc - tic)} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 相比 CPU 更擅长做 SIMD 计算，但只要用了向量化的代码，Numpy 就可以很好的并行化。\n",
    "\n",
    "经验法则是：只要有其他可能，就不要使用显示 for 循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 向量化的更多例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到显式 for loop 就可以想想看，能否向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 向量化 logistic 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{(1)} = w^T x^{(1)} + b$  \n",
    "$z^{(2)} = w^T x^{(2)} + b$    \n",
    "\n",
    "$a^{(1)} = \\delta(z^{(1)})$    \n",
    "$a^{(2)} = \\delta(z^{(2)})$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就需要计算每一个样本的预测值，但是定义成矩阵就不需要这么麻烦了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^{n_x m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z = [z^{(1)} z^{(2)}...z^{(m)}] = w^T X + [b b... b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横向堆叠成一个 1*m 的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而 Python 里面的广播在向量+实数时会自动把实数转为向量或矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 向量化 logistic 回归的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于梯度如何向量化呢？其实也是去定义个平展的向量\n",
    "\n",
    "$dz = [dz^{(1)}dz^{(2)}...dz^{(m)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = sigmoid(Z)\n",
    "\n",
    "待补图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 关于 Python 和 Numpy 的说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01667933  0.32822548 -0.51940435  0.48867466  0.91095995]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "a = np.random.randn(5)\n",
    "\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种 (5,) 的结构，这时所谓的 Python 秩(rank)为 1 的数组，他既不是行向量也不是列向量，这导致它又一些略微不直观的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01667933  0.32822548 -0.51940435  0.48867466  0.91095995]\n",
      "2.47980066535\n"
     ]
    }
   ],
   "source": [
    "print(a.T)\n",
    "print(np.dot(a,a.T))  # 期待广播是矩阵，但是结果是数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "吴恩达的建议是，编写神经网络时，最好不要使用这种秩的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[ 1.39434262]\n",
      " [-1.22466554]\n",
      " [-0.74398947]\n",
      " [ 0.59785129]\n",
      " [-0.77731506]]\n",
      "a transpose: [[ 1.39434262 -1.22466554 -0.74398947  0.59785129 -0.77731506]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5, 1)\n",
    "print('a:', a)\n",
    "print('a transpose:', a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.94419133 -1.70760336 -1.03737622  0.83360954 -1.08384352]\n",
      " [-1.70760336  1.4998057   0.91113827 -0.73216788  0.95195098]\n",
      " [-1.03737622  0.91113827  0.55352033 -0.44479507  0.57831422]\n",
      " [ 0.83360954 -0.73216788 -0.44479507  0.35742617 -0.46471882]\n",
      " [-1.08384352  0.95195098  0.57831422 -0.46471882  0.60421871]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不清楚自己的维度，可以用 assert(a.shape == (5, 1)) 这样的语句来确认，同样的如果最后得到一个秩为 1 的数组，可以 reshape 一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意的点：\n",
    "\n",
    "* 列向量写成：n\\*1\n",
    "* 行向量写成：1\\*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
