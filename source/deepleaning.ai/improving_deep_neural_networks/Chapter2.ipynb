{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mini-batch 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的应用是一个高度依赖经验的过程\n",
    "\n",
    "mini-batch 的概念很简单\n",
    "\n",
    "**符号约定：**\n",
    "\n",
    "$z^{[l]}$ 表示层数， $X^{\\{t\\}}$、$Y^{\\{t\\}}$ 表示 mini-batch。\n",
    "\n",
    "**epoch 的概念：**意味着一次遍历了训练集。\n",
    "\n",
    "待补图<>\n",
    "\n",
    "在全量梯度下降算法中，一个 epoch 只能让你做一次梯度下降，而在 mini-batch 梯度下降中，一个 epoch 可以让你做很多次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 理解 mini-batch  梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch gradient descent 每次迭代， cost 必然是在下降的，如果不是，那肯定出错了，但 mini-batch 不一定，因为每一次训练的不是一样的 mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要决定的是 mini-batch 的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing your mini-batch size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If mini-batch size = m: Batch gradient desent\n",
    "* If mini-batch size = 1: Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种极端的下降情况：\n",
    "\n",
    "待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机梯度下降永远也不会下降到最低点(停留)，而是回一直在最小值附近波动。而实际上，选择 Mini-batch 大小则在二者之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch gradient desent 的主要缺点在于当数据量很大的时候，训练速度会变得极慢，相反，如果使用随机梯度下降法，如果只需要处理 1 个样本，那么是没问题的，噪声可以通过学习率等的调整来改善，但随机梯度下降的一大缺点是，你会失去所有向量化的带来的加速，因为一次性只处理了一个训练样本，训练效率实在过于低下。\n",
    "\n",
    "所以，使用 mini-batch 有两个好处，一方面你得到了大量向量化，另一方面，你不需要等待整个训练集被处理完，就可以开始进行后续工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于三者的选择：\n",
    "\n",
    "* 如果你的数据集很小(少于 2000 个样本)，那么直接用 batch 梯度下降法\n",
    "* 如果数据量较大的话，一般 mini-batch 大小为 64 到 512，考虑到电脑内存设置和使用的方式，如果 mini-batch 是 2 次方，Mini-batch 的训练速度会快一些。另外，mini-batch 记得要符号 GPU/CPU 的内存。多做几次测试，才能找到最有效的减少成本函数的那个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 指数加权移动平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "例子的公式：某天的 v 等于前一天的 0.9 加上当天的 0.1.\n",
    "\n",
    "$$ v_t = \\beta v_{t-1} + (1-\\beta)\\theta _t$$\n",
    "\n",
    "$$average \\approx \\frac{1}{1-\\beta} day's$$\n",
    "\n",
    "比如 $\\beta$ 等于 0.9 则 $\\approx$ 10，0.98 -> 50，0.5 -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 理解指数加权平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_100 = 0.1 \\theta _{100} + 0.1 * 0.9 * \\theta _{99} + 0.1 * 0.9^2 \\theta _ {98} + ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据一些常数，你能大概知道能够平均多少日的温度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 指数加权平均的偏差修正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "偏差修正可以让平均数运算更加准确\n",
    "\n",
    "问题，待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{v_t}{1-\\beta ^t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你关心初始阶段的估计，那么在刚开始计算指数加权移动平均数的时候，偏差修正能帮助你在早期获得更好的估测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Momentum 梯度下降\n",
    "\n",
    "这种的运行速度几乎总是快于标准的梯度下降，核心思想就是计算梯度的指数加权平均数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降的波动，减慢了学习速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{dw} = \\beta v_{dw} + (1-\\beta)dw$$\n",
    "$$v_{db} = \\beta v_{db} + (1-\\beta)db$$\n",
    "\n",
    "$$w := w - \\alpha v_{dw}$$\n",
    "$$...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就能在纵轴方向放缓，而在横向方面加速，在抵达最小的路上，停止了摆动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在梯度下降中，就不需要偏差修正了，因为在迭代10次之后，就脱离初始状态了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了动量梯度下降法可以加快训练速度，还有个算法叫 RMSprop 也可以加快训练速度\n",
    "\n",
    "其实这个跟动量梯度下降很像，待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Adam 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam 算法是将 Momentum 和 RMSprop 结合在一起的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待补图<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 Adam 算法的时候，需要偏差修正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参的选择：\n",
    "\n",
    "* $\\alpha$： needs to tuned\n",
    "* $\\beta_1$：0.9(default) -> (dw)\n",
    "* $\\beta_2$：0.999(default) -> (dw^2)\n",
    "* $\\epsilon$：建议 10^{-8}，但没有那么重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 学习率衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 epoch = 1 pass through data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha  = \\frac{1}{1+decay-rate*epoch-num}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待补图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个也叫指数衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了指数衰减，还有些其他的衰减方式，如下图：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但指数衰减并不是首先要尝试的东西"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 局部最优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在高纬空间里，你更有可能碰到鞍点，而不是局部最优。可以想象下马鞍的鞍部。所以真正的问题不是居部最优，而是鞍部会减慢学习速度。这也是为啥 Adam 等算法能加速的原因。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
