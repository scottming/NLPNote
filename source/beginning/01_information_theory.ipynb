{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概率论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较熟悉可以跳过的概念(看一下 wiki 就可以搞懂)：\n",
    "\n",
    "* 条件概率\n",
    "* 贝叶斯法则(可从全概率公司去推导)\n",
    "* 随机变量\n",
    "* 二项式分布\n",
    "* 联合概率分布和条件概率分布\n",
    "* 期望和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略难懂的概念：\n",
    "\n",
    "* 极大似然估计\n",
    "* 贝叶斯决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 极大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本质：\n",
    "\n",
    "> 就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要在数学上实现最大似然估计法，我们首先要定义似然函数:\n",
    "\n",
    "$$ {\\displaystyle {\\mbox{lik}}(\\theta )=f_{D}(x_{1},\\dots ,x_{n}\\mid \\theta )} \\mbox{lik}(\\theta) = f_D(x_1,\\dots,x_n \\mid \\theta) $$\n",
    "\n",
    "并且在 ${\\displaystyle \\theta } \\theta$ 的所有取值上通过令一阶导数等于零，使这个函数取到最大值。这个使可能性最大的 ${\\displaystyle {\\widehat {\\theta }}}$ 值即称为 $\\theta$ 的最大似然估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 0.5, 0.4 来自方差 1 的正太分布，不知道均值，用似然函数估计均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_4 is 5.339962254753888e-07\n",
      "Log(L_4) is -14.442877066409347\n",
      "---------------\n",
      "L_3 is 0.00023808636632293787\n",
      "Log(L_3) is -8.342877066409345\n",
      "---------------\n",
      "L_2 is 0.014366199816936947\n",
      "Log(L_2) is -4.242877066409346\n",
      "---------------\n",
      "L_1 is 0.1173168287030054\n",
      "Log(L_1) is -2.142877066409345\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "L_4 = stats.norm(4, 1).pdf(0.5) * stats.norm(4, 1).pdf(0.4)\n",
    "print(f'L_4 is {L_4}')\n",
    "print(f'Log(L_4) is {np.log(L_4)}')\n",
    "print('---' * 5)\n",
    "\n",
    "L_3 = stats.norm(3, 1).pdf(0.5) * stats.norm(3, 1).pdf(0.4)\n",
    "print(f'L_3 is {L_3}')\n",
    "print(f'Log(L_3) is {np.log(L_3)}')\n",
    "print('---' * 5)\n",
    "\n",
    "L_2 = stats.norm(2, 1).pdf(0.5) * stats.norm(2, 1).pdf(0.4)\n",
    "print(f'L_2 is {L_2}')\n",
    "print(f'Log(L_2) is {np.log(L_2)}')\n",
    "print('---' * 5)\n",
    "\n",
    "L_1 = stats.norm(1, 1).pdf(0.5) * stats.norm(1, 1).pdf(0.4)\n",
    "print(f'L_1 is {L_1}')\n",
    "print(f'Log(L_1) is {np.log(L_1)}')\n",
    "print('---' * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Understanding ML](https://pdfs.semanticscholar.org/2a3b/52782e00f8d8781fe1c0493627172063dca5.pdf)\n",
    "* [最大似然估计 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)\n",
    "* [如何通俗地理解概率论中的“极大似然估计法”? ](https://www.zhihu.com/question/24124998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**应用：**\n",
    "\n",
    "* 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 信息论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "易懂概念：\n",
    "\n",
    "* 熵\n",
    "* 联合熵和条件熵\n",
    "* 互信息\n",
    "* 相对熵\n",
    "\n",
    "略难懂却常用的概念\n",
    "\n",
    "* 交叉熵\n",
    "* 困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [熵](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA):\n",
    "\n",
    "#### 定义：\n",
    "\n",
    "如果随机离散变量的概率密度函数为 $p(x)$, 那么 $X$ 的定义为：\n",
    "\n",
    "$$ H(X) = - \\sum_{x=i}^n p(x)log_2p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**熵的本质：**\n",
    "\n",
    "* 信息熵的本质是信息量的期望。\n",
    "* 信息熵是对随机变量不确定性的度量。随机变量 X 的熵越大，说明他的不确定性也越大。若随机变量退化为定值，则熵为 0。\n",
    "* 平均分布是「最不确定的熵」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例子1：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记 X、Y、Z 分别为掷硬币、掷骰子、54张牌随机抽取一张的结果，3 个随机变量的熵之间关系为\n",
    "\n",
    "$$ H(X) < H(Y) < H(Y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例子2：**\n",
    "\n",
    "假定有 8 匹马参加一场赛马比赛，设 8 匹马的获胜概率分布为(1/2, 1/4, 1/8, 1/16, 1/64, 1/64, 1/64, 1/64)，求该场赛马的熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该场赛马的的熵为 2.0 比特。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_entropy(p): \n",
    "    entropy = -(p * np.log2(p))\n",
    "    return entropy\n",
    "\n",
    "horse_probabilities = [1/2, 1/4, 1/8, 1/16, 1/64, 1/64, 1/64, 1/64]\n",
    "horse_entropies = [compute_entropy(i) for i in horse_probabilities]\n",
    "print(f'该场赛马的的熵为 {sum(horse_entropies)} 比特。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大熵在语言模型中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考：\n",
    "\n",
    "[信息熵是什么？](https://www.zhihu.com/question/22178202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 联合熵和条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "联合，即从一个到扩展到两个，公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathrm{H} (X,Y)=-\\sum _{{x}}\\sum _{{y}}P(x,y)\\log _{2}[P(x,y)]\\!$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 [联合分布](https://zh.wikipedia.org/wiki/%E8%81%94%E5%90%88%E5%88%86%E5%B8%83) 到 [联合熵](https://zh.wikipedia.org/wiki/%E8%81%94%E5%90%88%E7%86%B5)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "三者的关系，待补图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他条件熵和联合熵请参考 <信息论> p24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵\n",
    "\n",
    "用来衡量估计模型与真实概率分布之间的差异情况\n",
    "\n",
    "[Deeplearning ch3 作业](https://render.githubusercontent.com/view/ipynb?commit=49b0082bf5f6a5f3bb19f8dbb8492f50d4c1d838&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73636f74746d696e672f446565704c6561726e696e673130312f343962303038326266356636613566336262313966386462623834393266353064346331643833382f6368332f636f64652f6c696e6561725f72656772657373696f6e2e6970796e623f746f6b656e3d414d50474d502d496d556c535a4c724d78506758334c734872733145616a6e4d6b73355a684749307741253344253344&nwo=scottming%2FDeepLearning101&path=ch3%2Fcode%2Flinear_regression.ipynb&repository_id=83048661&repository_type=Repository#2.-%E8%A7%A3%E9%87%8A%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设计语言模型是，通常用困惑度来代替交叉熵衡量语言模型的好坏。\n",
    "\n",
    "当你说一句话时，这句话最可能出现，信息量就大，即熵高。容易出现，熵就小。所以困惑度最小，也就最接近最真实的语言了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(entropy):\n",
    "    perplexity = 2**(entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例子1：**\n",
    "\n",
    "[What is Machine Learning Perplexity? | James D. McCaffrey](https://jamesmccaffrey.wordpress.com/2016/08/16/what-is-machine-learning-perplexity/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "prediction         targets\n",
    "-----------------------------------\n",
    "0.10  0.20  0.70   0  0  1   (correct)\n",
    "0.10  0.70  0.20   0  1  0   (correct)\n",
    "0.30  0.40  0.30   1  0  0   (wrong)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1 = [0.1, 0.2, 0.7]\n",
    "m2 = [0.1, 0.7, 0.2]\n",
    "m3 = [0.3, 0.4, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropies = [sum(compute_entropy(i) for i in j)\n",
    "             for j in (m1, m2, m3)]\n",
    "perplexities = [compute_perplexity(i) for i in entropies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1's perplexity is 2.23.\n",
      "m2's perplexity is 2.23.\n",
      "m3's perplexity is 2.97.\n"
     ]
    }
   ],
   "source": [
    "for s, p in zip('m1 m2 m3'.split(), perplexities):\n",
    "    print(f\"{s}'s perplexity is {p:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Another reason why SVMs enjoy high popularity among machine learning practitioners is that they can be easily kernelized to solve nonlinear classification problems.\n",
    "> -- Python Machine Learning book p75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先升维，再降维：[Solving non-linear problems using a kernel SVM](http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb#Solving-non-linear-problems-using-a-kernel-SVM)\n",
    "\n",
    "$$\\Phi(x_1, x_2) = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To solve a nonlinear problem using an SVM, we transform the training data onto a higher dimensional feature space via a mapping function φ ( ⋅ ) and train a linear SVM model to classify the data in this new feature space. Then we can use the same mapping function φ ( ⋅ ) to transform new, unseen data to classify it using the linear SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Roughly speaking, the term kernel can be interpreted as a similarity function between a pair of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核技巧，这点暂时没搞清楚具体的意义，我的理解是为了降低计算复杂度的东西"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大熵模型\n",
    "\n",
    "暂未搞懂和推导"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
