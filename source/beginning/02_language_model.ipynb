{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个由 l 个词构成的句子的概率计算公式为：\n",
    "\n",
    "$$ p(s) = p(w_1)p(w_2|w_1)p(w_3|w_1 w_2)...p(w_l|w_1..w_{l-1}) \\\\\n",
    "        = \\prod_{i=1}^l p(w_i | w_1 ...w_{i-1}) \\\\\n",
    "        \\approx \\prod_{i=1}^l p(w_i | wi-1)        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面最后一个例子是二元模型的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么在 bigram 下，给定一个前文 x, y 的概率是多少呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n)}{\\sum_w C(w_{n-1}w)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实这个式子等于："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict, deque, Counter\n",
    "import json\n",
    "import random \n",
    "import nltk\n",
    "from nltk import ngrams, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: [\"It's\", 'good', 'to', 'see', 'you', 'you.']\n",
      "ngram: [(\"It's\", 'good'), ('good', 'to'), ('to', 'see'), ('see', 'you'), ('you', 'you.')]\n",
      "freq:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({(\"It's\", 'good'): 1,\n",
       "          ('good', 'to'): 1,\n",
       "          ('see', 'you'): 1,\n",
       "          ('to', 'see'): 1,\n",
       "          ('you', 'you.'): 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"It's good to see you you.\".split()\n",
    "words_ngram = list(ngrams(words, 2))\n",
    "words_freq = FreqDist(words_ngram)\n",
    "\n",
    "print('words:', words)\n",
    "print('ngram:', words_ngram)\n",
    "print('freq:\\n')\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import jieba\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n",
    "\n",
    "def segment_data(data):\n",
    "    \"\"\"segment data to sentence.\"\"\"\n",
    "    sentence_break = r'(。|！|？)'\n",
    "    for line in data:\n",
    "        # 按句子切分，并且捕获分组\n",
    "        sentences = re.split(sentence_break, line)\n",
    "        # 捕获的 delimiter 添加到字符串后面\n",
    "        sentences = itertools.zip_longest(sentences[0::2], \n",
    "                                          sentences[1::2],\n",
    "                                          fillvalue='')\n",
    "        sentences = (''.join(i) for i in sentences)\n",
    "        yield sentences\n",
    "\n",
    "def jieba_cut(sentences):\n",
    "    # Flat the nested iterator\n",
    "    sentences = itertools.chain.from_iterable(sentences)\n",
    "    # Clean empty sentence\n",
    "    sentences = (sentence for sentence in sentences if sentence)\n",
    "    for sentence in sentences:\n",
    "        sentence = deque(jieba.cut(sentence))\n",
    "        sentence.appendleft('$$')\n",
    "        sentence = ' '.join(sentence)\n",
    "        yield sentence\n",
    "\n",
    "def get_ngram(sentences, n=2):\n",
    "    \"\"\"Get ngram from a list of string.\"\"\"\n",
    "    for sentence in sentences:\n",
    "        ngram = ngrams(sentence.split(), n)\n",
    "        yield ngram\n",
    "        \n",
    "def count_ngram(ngram, n=2):\n",
    "    d = defaultdict(Counter)\n",
    "    for context, value in ngram:\n",
    "        d[context][value] += 1\n",
    "    return d\n",
    "\n",
    "def count_to_prob(dct):\n",
    "    prob_dct = dct.copy()\n",
    "    for context, count in prob_dct.items():\n",
    "        total = sum(count.values())\n",
    "        for word in count:\n",
    "            count[word] /= total\n",
    "    return prob_dct\n",
    "\n",
    "def generate_word(context):\n",
    "    psum = 0\n",
    "    r = random.random()\n",
    "    for word, prob in ngram_prob[context].items():\n",
    "        psum += prob\n",
    "        if psum > r:\n",
    "            return word\n",
    "\n",
    "def generate_sentence(word):\n",
    "    while word:\n",
    "        word = generate_word(word)\n",
    "        yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/df/g_gpcyqx3j35s2yvr5k2tysc0000gn/T/jieba.cache\n",
      "Loading model cost 1.308 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "data = read_data('data/YGZ-rain.md')\n",
    "sentences = segment_data(data)\n",
    "sentences = jieba_cut(sentences)\n",
    "ngram = itertools.chain.from_iterable(get_ngram(sentences, n=2))\n",
    "ngram_counts = count_ngram(ngram)\n",
    "ngram_prob = count_to_prob(ngram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不过整个太平洋只为向他听的淡淡土腥气，各成世界小得多可爱，而且躲在一起。\n"
     ]
    }
   ],
   "source": [
    "g_sentence = generate_sentence('$$')\n",
    "sentence = ''.join(more_itertools.islice_extended(g_sentence, 0, -1))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 个词时候的探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', \"don't\", 'master'),\n",
       " (\"don't\", 'master', 'regular'),\n",
       " ('master', 'regular', 'expression'),\n",
       " ('regular', 'expression', \"don't\"),\n",
       " ('expression', \"don't\", 'master'),\n",
       " (\"don't\", 'master', 'regular'),\n",
       " ('master', 'regular', 'expression')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list(ngrams(\"I don't master regular expression don't master regular expression\".split(), 3))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\"] master\n",
      "[\"don't\", 'master'] regular\n",
      "['master', 'regular'] expression\n",
      "['regular', 'expression'] don't\n",
      "['expression', \"don't\"] master\n",
      "[\"don't\", 'master'] regular\n",
      "['master', 'regular'] expression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {('I', \"don't\"): Counter({'master': 1}),\n",
       "             (\"don't\", 'master'): Counter({'regular': 2}),\n",
       "             ('expression', \"don't\"): Counter({'master': 1}),\n",
       "             ('master', 'regular'): Counter({'expression': 2}),\n",
       "             ('regular', 'expression'): Counter({\"don't\": 1})})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(Counter)\n",
    "for *i, j in test:\n",
    "    d[tuple(i)][j] += 1\n",
    "    print(i, j)\n",
    "    \n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平滑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平滑的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 平滑的背景\n",
    "* 平滑的分类\n",
    "    + Add one\n",
    "    + Add k\n",
    "    + back off / interpolation\n",
    "    + KN\n",
    "* 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpolation 相比 back off 效果要好一些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 Held_out 去设置我们的 lambada，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add-one(Laplace) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于unigram模型而言，会有 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{add1}(w_i) = \\frac{C(w_i)+1}{M+|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，M 是训练语料中所有的N-Gram的数量（token），而 V 是所有的可能的不同的N-Gram的数量（type）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，对于bigram模型而言，可得:\n",
    "\n",
    "$$P_{add1}(w_i|w_{i-1}) = \\frac{C(w_{i-1}w_i)+1}{C(w_{i-1})+|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推而广之，对于n-Gram模型而言，可得:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{add1}(w_i|w_{i-n+1},\\cdots,w_{i-1})=\\frac{C(w_{i-n+1},\\cdots,w_i)+1}{C(w_{i-n+1},\\cdots,w_{i-1})+|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-k Smoothing（Lidstone’s law）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add k 很简单，就是在 Add-one 的 one 位置 * k，概率公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{addk}(w_i|w_{i-n+1}\\cdots w_{i-1}) = \\frac{C(w_{i-n+1}\\cdots w_i)+k}{C(w_{i-n+1}\\cdots w_{i-1})+k|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回退的概念是：在高阶模型上找不到是，则回退到低价模型取概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{backoff}(w_i|w_{i-n+1}\\cdots w_{i-1})=\\begin{cases}\n",
    "P^*(w_i|w_{i-n+1}\\cdots w_{i-1})&,if\\quad C(w_{i-n+1}\\cdots w_i)>0\\\\\n",
    "\\alpha (w_{i-n+1}\\cdots w_{i-1})\\cdot P_{backoff}(w_i|w_{i-n+2}\\cdots w_{i-1})&,otherwise\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "插值算法就是把不同阶别的 n-Gram 模型线性加权组合后再用，公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{interp}(w_n|w_{n-2},w_{n-1})=\\lambda_1P(w_n)+\\lambda_2P(w_n|w_{n-1})+\\lambda_3P(w_n|w_{n-2},w_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $0≤λi≤1，∑iλi=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute Discounting 的思想其实跟 Add-one 和 Add-k 差不多, 具体可参考下面的 refrences。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneser-Ney Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看懂了接续频率会让整个概率变小，但具体的公式暂时没看懂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram 平滑总结：\n",
    "\n",
    "* Add-1 smoothing:\n",
    "    + ok for text categorization, not for language modeling\n",
    "* The most commonly used method:\n",
    "    + Extended interpolated Kneser-Ney\n",
    "* For very Large N-grams like the Web:\n",
    "    * Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf](https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf)\n",
    "* [自然语言处理中N-Gram模型的Smoothing算法 - 白马负金羁 - CSDN博客](http://blog.csdn.net/baimafujinji/article/details/51297802)\n",
    "* [初涉自然语言处理（五）：平滑处理 | Vkki](http://vkki.me/2016/03/18/nlp-5/)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
